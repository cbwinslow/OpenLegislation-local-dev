# Bulk XML Ingestion Plan

This document outlines how to connect the publicly available bulk XML feeds from
Congress.gov and GovInfo with the Java data model and Flyway migrations that ship
with OpenLegislation.

## 1. Inventory of relational targets

The `SqlTable` enumeration enumerates every table that the Java DAO layer touches.
A helper script (`tools/generate_table_mapping.py`) cross references that enum with
the Flyway migrations located in `src/main/resources/sql/migrations`. Running the
script produces a JSON or Markdown report showing which migration created each
table and whether any entries are missing on either side.

```bash
./tools/generate_table_mapping.py --format markdown --output docs/sqltable_mapping.md
```

The report establishes the schema contract that downstream ingestion must satisfy.

## 2. Bulk data acquisition

The new `tools/bulkdata_pipeline.py` module downloads XML payloads from both data
providers. It requires API keys which can be supplied via the environment or command
line. Example usage that performs a focused pull across a couple of congresses and
collections:

```bash
export CONGRESS_GOV_API_KEY="<your congress.gov api key>"
export GOVINFO_API_KEY="<your govinfo api key>"
./tools/bulkdata_pipeline.py \
  --output-dir data/bulk-xml \
  --congress 118 117 \
  --govinfo-collections BILLS BILLSTATUS \
  --govinfo-from "2023-01-01" \
  --govinfo-page-size 500 \
  -v
```

Key features:

* Unified retrying HTTP client that copes with throttling.
* Separate downloaders for the Congress.gov and GovInfo APIs.
* Optional aggregation step that emits a single `aggregate.xml` combining every
  document into one file (useful when a downstream loader expects a single XML
  payload).
* Configurable page sizes and per-endpoint throttling to respect API rate
  policies without editing the source code.
* A queue planner (`--plan-output`) that writes a JSON worklist describing every
  page fetch (for both APIs and GovInfo bulk archives). The generated plan can be
  executed later via `--plan-input plan.json`, allowing long running imports to
  be chunked and resumed safely.

## 3. Mapping XML to the Java domain

* The XML downloaded from the APIs can be parsed using the existing JAXB/SAX
  utilities under `src/main/java/gov/nysenate/openleg/`. Those classes already map
  elements such as bills, actions, committees, and calendars into the domain model.
* The `SqlTable` enum coupled with DAO query classes (e.g. classes ending with
  `Dao` or `Query`) define how domain objects persist into PostgreSQL.
* Developers can iterate on a per-table basis by inspecting the generated mapping
  report and locating the DAO or service that owns a given table.

## 4. Loading workflow

1. Use `tools/bulkdata_pipeline.py` to populate the raw XML staging directory.
2. For each XML document, feed it through the appropriate processor in
   `src/main/java/gov/nysenate/openleg/processors` or reuse the existing ingestion
   trackers under `tools/` for incremental processing.
3. Verify the relational state by comparing table contents against the schema
   report generated by `tools/generate_table_mapping.py`.

## 5. Next steps

* Extend `tools/bulkdata_pipeline.py` with checksum verification for long-running
  bulk exports.
* Wire the downloader into the existing ingestion trackers (`tools/manage_all_ingestion.py`)
  so that new data automatically queues for processing.
* Consider adding automated tests that run the mapping script to guard against
  missing migrations when new tables are introduced.
